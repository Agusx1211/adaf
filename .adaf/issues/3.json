{
  "id": 3,
  "title": "Model performance ranking system with task categorization",
  "description": "## Summary\n\nImplement a system where manager/lead agents rank the output quality of worker agents after each task, combined with task categorization, to build per-model performance profiles over time.\n\n## Motivation\n\nCurrently, agent selection is static — the user or orchestrator picks a model without empirical data on how well each model performs on different kinds of work. By capturing quality signals at the natural evaluation point (when a manager reviews a worker's output), we can build a data-driven routing system that learns which models excel at which tasks.\n\n## Design\n\n### 1. Task Categorization\n\nWhen a manager agent spawns a sub-agent, it must assign a task category. This provides the dimension along which we measure model strengths.\n\n**Proposed initial taxonomy** (start broad, refine later):\n- `refactor` — restructuring existing code without changing behavior\n- `bugfix` — diagnosing and fixing defects\n- `feature` — implementing new functionality (greenfield)\n- `test` — writing or updating tests\n- `docs` — documentation, comments, READMEs\n- `investigation` — research, codebase exploration, root cause analysis\n- `review` — code review, quality assessment\n- `config` — configuration, CI/CD, build system changes\n\nThe category should be a required field when spawning a sub-agent via the orchestrator. Store it in the spawn metadata.\n\n### 2. Quality Ranking\n\nAfter a worker completes a task, the manager agent provides a structured quality assessment. Two possible approaches:\n\n**Option A: Simple score (recommended to start)**\n- Single 1-5 integer score\n- Lightweight, easy to aggregate\n- Loses nuance but accumulates signal fast\n\n**Option B: Multi-dimensional (future evolution)**\n- `correctness` (1-5): Did it solve the problem?\n- `code_quality` (1-5): Is the code clean and idiomatic?\n- `task_adherence` (1-5): Did it follow instructions precisely?\n- `completeness` (1-5): Is the work thorough?\n\nStart with Option A. Evolve to Option B once we have enough data to validate the system.\n\n### 3. Data Storage\n\nStore rankings in the project store (`.adaf/rankings/` or similar):\n```json\n{\n  \"id\": 42,\n  \"turn_id\": 7,\n  \"worker_agent\": \"gemini\",\n  \"worker_model\": \"gemini-2.5-pro\",\n  \"evaluator_agent\": \"claude\",\n  \"evaluator_model\": \"claude-sonnet-4-5-20250929\",\n  \"task_category\": \"bugfix\",\n  \"score\": 4,\n  \"timestamp\": \"2026-02-12T...\",\n  \"plan_id\": \"default\",\n  \"notes\": \"Fixed the issue correctly but left some dead code\"\n}\n```\n\n### 4. Aggregation \u0026 Statistics\n\nProvide a CLI command (`adaf stats models` or similar) that computes:\n- Per-model average score (overall and per-category)\n- Number of ranked tasks per model per category (confidence indicator)\n- Trend over time (are scores improving/degrading?)\n\n### 5. Adaptive Routing (future)\n\nOnce enough data accumulates:\n- Expose aggregated stats to manager agents via the protocol/system prompt\n- Agents can use this to make informed spawning decisions\n- Consider an explore/exploit strategy: occasionally route tasks to lower-ranked models to keep data fresh and avoid starving them of opportunities\n\n### 6. Evaluator Bias Normalization\n\nDifferent evaluator models may grade differently (some harsh, some lenient). Consider normalizing scores per-evaluator so that a \"4 from Claude\" and a \"4 from Gemini\" mean the same thing.\n\n## Open Questions\n\n- Should the ranking be mandatory or opt-in per spawn?\n- How many data points per model/category before we consider the stats reliable?\n- Should we store the raw worker output alongside the ranking for later re-evaluation?\n- How to handle the cold start problem? Use agentmeta catalog capabilities as priors?\n\n## Implementation Notes\n\n- The natural integration point is in `internal/orchestrator/` where spawns happen and results come back\n- Rankings data can follow the same store pattern as issues (JSON files in a directory)\n- The stats command would live in `internal/cli/`\n- Protocol changes in `pkg/protocol/` to expose stats to agents",
  "status": "open",
  "priority": "medium",
  "labels": [
    "feature",
    "orchestrator",
    "analytics"
  ],
  "created": "2026-02-12T19:35:08.242540687Z",
  "updated": "2026-02-12T19:35:08.242540687Z"
}